library(gcookbook)
df_4a<- heightweight
df_4a["heightFt"] <- df_4a["heightIn"] / 12
model_4_in <- lm(weightLb ~ heightIn, data=df_4a)
model_4_ft <- lm(weightLb ~ heightFt, data=df_4a)
model_4_combine <- lm(weightLb ~ heightFt+heightIn, data=df_4a)
rm(list = ls())
library(gcookbook)
df_4a<- heightweight
df_4a["heightFt"] <- df_4a["heightIn"] / 12
model_4_in <- lm(weightLb ~ heightIn, data=df_4a)
model_4_ft <- lm(weightLb ~ heightFt, data=df_4a)
model_4_combine <- lm(weightLb ~ heightFt+heightIn, data=df_4a)
View(model_4_combine)
summary(model_4_in)
summary(model_4_ft)
View(df_4a)
model_4_ft <- lm(weightLb ~ heightFt, data=df_4a)
model_4_combine <- lm(weightLb ~ heightFt+heightIn, data=df_4a)
summary(model_4_combine)
mean(model_4_in$residuals^2)
mean(model_4_ft$residuals^2)
mean(model_4_combine$residuals^2)
c <- lm(formula = Auto$mpg ~ Auto$horsepower + Auto$origin)
c$coefficients
library(gcookbook)
heightweight["heightFt"] <- heightweight["heightIn"] / 12
model_4_in <- lm(weightLb ~ heightIn, data=heightweight)
model_4_ft <- lm(weightLb ~ heightFt, data=heightweight)
model_4_combine <- lm(weightLb ~ heightFt+heightIn, data=heightweight)
mean(model_4_in$residuals^2)
mean(model_4_ft$residuals^2)
mean(model_4_combine$residuals^2)
val <- heightweight["heightIn"] / 12
heightweight["heightFt"] <- val
model_4_in <- lm(weightLb ~ heightIn, data=heightweight)
model_4_ft <- lm(weightLb ~ heightFt, data=heightweight)
model_4_combine <- lm(weightLb ~ heightFt+heightIn, data=heightweight)
mean(model_4_in$residuals^2)
mean(model_4_ft$residuals^2)
mean(model_4_combine$residuals^2)
############################## 5A
redmean = matrix(c(0,0), nrow = 2, ncol=1)
redsd = diag(2)
bluemean = matrix(c(1.5, 1.5), nrow = 2, ncol=1)
bluesd = diag(2)
n = 25
x1 <- matrix(rnorm(n, mean=redmean, sd=redsd))
y1 <- matrix(rnorm(n, mean=redmean, sd=redsd))
red <- cbind(x1, y1)
x2 <- matrix(rnorm(n, mean=bluemean, sd=bluesd))
y2 <- matrix(rnorm(n, mean=bluemean, sd=bluesd))
blue <- cbind(x2, y2)
plot(red, col="red", xlim = c(-4, 4), ylim= c(-4,4), xlab = "x1, x2 values", ylab = "y1, y2 values", main = "5a")
points(blue, col="blue")
legend('bottomright', c('red', 'blue'), col = c('red', 'blue'), pch=1)
########################### 5B
# testb <- sample(blue,25)
# testr <- sample(red, 25)
set.seed(10)
testbx <- sample(x2, 25)
testby <- sample(y2, 25)
testb <- cbind(testbx, testby)
testrx <- sample(x1, 25)
testry <- sample(y1, 25)
testr <- cbind(testrx, testry)
totalTesting <- rbind(testb, testr)
totalTraining <- rbind(blue, red)
plot(red, col="red", xlab = "x1, x2 values", ylab="y1, y2 values", main = "5b", xlim= c(-4, 4), ylim = c(-4, 4))
points(blue, col="blue")
points(testr, col="red", pch=0)
points(testb, col="blue", pch=0)
legend('bottomright', c( 'Testing Set', 'Training Set', 'Red', 'Blue'), pch = c(0,1, 3, 3), col = c('Black','Black', 'Red', 'Blue'))
########################### 5C
clb <- rep("blue", times = 25)
clr <- rep("red", times = 25)
cl <- cbind(clb, clr)
# totalTraining <- rbind(blue, red)
# totalTesting <- rbind(testr, testb)
# length(blue)
# length(testb)
# fit <- knn(totalTraining, totalTesting, cl, k=1)
# length(fit)
# cm = as.matrix(table(Actual = clb, Predicted = fit))
#
# accuracy <- sum(diag(cm))/length(clb)
# accuracy
clb <- rep("blue", times = 25)
total_errors = c()
for (val in 1:20) {
fit = knn(totalTraining, totalTesting, cl, k=val)
fit
cm = as.matrix(table(Actual = cl, Predicted = fit))
accuracy <- sum(diag(cm)) / length(cl)
error <- 1 - accuracy
total_errors <- append(total_errors, error)
error
}
onek <- c()
onek <- append(onek, 1/1:20)
total_errors[1] = .125
print(total_errors)
plot(onek, total_errors, type="o", xlab = "1/k", ylab="classification error")
############### 5D
plot(testr, col="red", xlab = "x1, x2 values", ylab="y1, y2 values", main = "5d", xlim= c(-4, 4), ylim = c(-4, 4))
points(testb, col="blue")
points(missed[1:2], col="blue", pch=4)
points(missed[3:4], col="red", pch=4)
legend('bottomright', c( 'Red', 'Blue', 'Mislabeled (color is what it is supposed to be)'), pch = c(3, 3, 4), col = c('Red', 'Blue', 'Red'))
###################### 5E
plot(testr, col="red", xlab = "x1, x2 values", ylab="y1, y2 values", main = "5d", xlim= c(-4, 4), ylim = c(-4, 4))
points(testb, col="blue")
points(missed[1:2], col="blue", pch=4)
points(missed[3:4], col="red", pch=4)
legend('bottomright', c( 'Red', 'Blue', 'Mislabeled (color is what it is supposed to be)'), pch = c(3, 3, 4), col = c('Red', 'Blue', 'Red'))
##################### 7A
create_dist <- function(seeds) {
library(MASS)
library(ggplot2)
set.seed(seeds)
mu_1 = c(0,0)
sigma = matrix(c(1,0,0,1), 2)
class_1 = mvrnorm(50, mu=mu_1, Sigma=sigma)
colnames(class_1) <- c("x1","x2")
mu_2 = c(5,5)
class_2 = mvrnorm(50, mu=mu_2, Sigma=sigma)
colnames(class_2) <- c("x1","x2")
mu_3 = c(10,10)
class_3 = mvrnorm(50, mu=mu_3, Sigma=sigma)
colnames(class_3) <- c("x1","x2")
all_x = append(append(class_1[,1], class_2[,1]), class_3[,1])
all_y = append(append(class_1[,2], class_2[,2]), class_3[,2])
testing_class_labels = append(append(rep("1", 50), rep("2", 50)), rep("3", 50))
df_7a = data.frame(x=all_x, y=all_y, actual=testing_class_labels)
all_vals = cbind(all_x, all_y)
return(df_7a)
}
training <- create_dist(123)
testing <- create_dist(456)
##################### 7B
##################### 7C
first <- training[training$actual == 1,]
second <- training[training$actual == 2,]
third <- training[training$actual == 3,]
plot(first$x, first$y, col = "red", xlim = c(-5, 15), ylim = c(-15, 15), xlab = "x values", ylab = "y values")
points(second$x, second$y, col = "blue")
points(third$x, third$y, col = "green")
library(klaR)
partimat(all_vals ~ x + y, data = df_7a, method ="lda")
mdl <- lda(training$actual ~ x + y, data = training)
pred_mdl <- predict(mdl, testing[,-3])
cm = as.matrix(table(Actual = training$actual, Predicted = pred_mdl$class))
cm
##############################################
df_7a <- create_dist(456)
model_7a <- lda(actual ~ x + y, data=df_7a)
datPred = data.frame(Class=predict(model_7a)$class,predict(model_7a)$x)
model_7a2 <- lda(datPred[,2:3], datPred[,1])
x <- seq(min(datPred[,2]), max(datPred[,2]), length.out=150)
y <- seq(min(datPred[,3]), max(datPred[,3]), length.out=150)
Xcon <- matrix(c(rep(x,length(y)), rep(y, rep(length(x), length(y)))),,2)
model_post1 <- predict(model_7a2, Xcon)$post[, c("1","2")] %% c(1,1)    #posterior probabilities of a point belonging to each class
model_post2 <- predict(model_7a2, Xcon)$post[, c("2","3")] %% c(1,1)
pr<-data.frame(x=rep(x, length(y)), y=rep(y, each=length(x)), z1=as.vector(model_post1), z2=as.vector(model_post2))
ggplot(datPred, aes(x = LD1, y = LD2)) + geom_point(size=3, aes(pch=Class, col=Class)) + geom_contour(data=pr, aes(x=x, y=y, z=z1), breaks=c(0,.5)) + geom_contour(data=pr, aes(x=x, y=y, z=z2), breaks=c(0,.5))
###################################### 8A
first <- training[training$actual == 1,]
second <- training[training$actual == 2,]
third <- training[training$actual == 3,]
plot(first$x, first$y, col = "red", xlim = c(-5, 15), ylim = c(-15, 15), xlab = "x values", ylab = "y values")
points(second$x, second$y, col = "blue")
points(third$x, third$y, col = "green")
library(klaR)
partimat(all_vals ~ x + y, data = df_7a, method ="lda")
mdl <- qda(training$actual ~ x + y, data = testing)
pred_mdl <- predict(mdl, testing[,-3])
cm = as.matrix(table(Actual = training$actual, Predicted = pred_mdl$class))
cm
#########################################
boundary <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
### guess how to get class labels from predict
### (unfortunately not very consistent between models)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
model_7a <- qda(actual ~ x + y, data=training)
boundary(model_7a, training, class="actual")
##################### 7A
create_dist <- function(seeds) {
library(MASS)
library(ggplot2)
set.seed(seeds)
mu_1 = c(0,0)
sigma = matrix(c(1,0,0,1), 2)
class_1 = mvrnorm(50, mu=mu_1, Sigma=sigma)
colnames(class_1) <- c("x1","x2")
mu_2 = c(5,5)
class_2 = mvrnorm(50, mu=mu_2, Sigma=sigma)
colnames(class_2) <- c("x1","x2")
mu_3 = c(10,10)
class_3 = mvrnorm(50, mu=mu_3, Sigma=sigma)
colnames(class_3) <- c("x1","x2")
all_x = append(append(class_1[,1], class_2[,1]), class_3[,1])
all_y = append(append(class_1[,2], class_2[,2]), class_3[,2])
testing_class_labels = append(append(rep("1", 50), rep("2", 50)), rep("3", 50))
df_7a = data.frame(x=all_x, y=all_y, actual=testing_class_labels)
all_vals = cbind(all_x, all_y)
return(df_7a)
}
training <- create_dist(123)
testing <- create_dist(456)
##################### 7B
##################### 7C
first <- training[training$actual == 1,]
second <- training[training$actual == 2,]
third <- training[training$actual == 3,]
plot(first$x, first$y, col = "red", xlim = c(-5, 15), ylim = c(-15, 15), xlab = "x values", ylab = "y values")
points(second$x, second$y, col = "blue")
points(third$x, third$y, col = "green")
library(klaR)
partimat(all_vals ~ x + y, data = df_7a, method ="lda")
mdl <- lda(training$actual ~ x + y, data = training)
pred_mdl <- predict(mdl, testing[,-3])
cm = as.matrix(table(Actual = training$actual, Predicted = pred_mdl$class))
cm
##############################################
df_7a <- create_dist(456)
model_7a <- lda(actual ~ x + y, data=df_7a)
datPred = data.frame(Class=predict(model_7a)$class,predict(model_7a)$x)
model_7a2 <- lda(datPred[,2:3], datPred[,1])
x <- seq(min(datPred[,2]), max(datPred[,2]), length.out=150)
y <- seq(min(datPred[,3]), max(datPred[,3]), length.out=150)
Xcon <- matrix(c(rep(x,length(y)), rep(y, rep(length(x), length(y)))),,2)
model_post1 <- predict(model_7a2, Xcon)$post[, c("1","2")] %% c(1,1)    #posterior probabilities of a point belonging to each class
model_post2 <- predict(model_7a2, Xcon)$post[, c("2","3")] %% c(1,1)
pr<-data.frame(x=rep(x, length(y)), y=rep(y, each=length(x)), z1=as.vector(model_post1), z2=as.vector(model_post2))
ggplot(datPred, aes(x = LD1, y = LD2)) + geom_point(size=3, aes(pch=Class, col=Class)) + geom_contour(data=pr, aes(x=x, y=y, z=z1), breaks=c(0,.5)) + geom_contour(data=pr, aes(x=x, y=y, z=z2), breaks=c(0,.5))
###################################### 8A
first <- training[training$actual == 1,]
second <- training[training$actual == 2,]
third <- training[training$actual == 3,]
plot(first$x, first$y, col = "red", xlim = c(-5, 15), ylim = c(-15, 15), xlab = "x values", ylab = "y values")
points(second$x, second$y, col = "blue")
points(third$x, third$y, col = "green")
library(klaR)
partimat(all_vals ~ x + y, data = df_7a, method ="lda")
mdl <- qda(training$actual ~ x + y, data = testing)
pred_mdl <- predict(mdl, testing[,-3])
cm = as.matrix(table(Actual = training$actual, Predicted = pred_mdl$class))
cm
#########################################
boundary <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
### guess how to get class labels from predict
### (unfortunately not very consistent between models)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
model_7a <- qda(actual ~ x + y, data=training)
boundary(model_7a, training, class="actual")
df_7a <- create_dist(456)
model_7a <- lda(actual ~ x + y, data=df_7a)
datPred = data.frame(Class=predict(model_7a)$class,predict(model_7a)$x)
model_7a2 <- lda(datPred[,2:3], datPred[,1])
x <- seq(min(datPred[,2]), max(datPred[,2]), length.out=150)
y <- seq(min(datPred[,3]), max(datPred[,3]), length.out=150)
Xcon <- matrix(c(rep(x,length(y)), rep(y, rep(length(x), length(y)))),,2)
model_post1 <- predict(model_7a2, Xcon)$post[, c("1","2")] %% c(1,1)    #posterior probabilities of a point belonging to each class
model_post2 <- predict(model_7a2, Xcon)$post[, c("2","3")] %% c(1,1)
pr<-data.frame(x=rep(x, length(y)), y=rep(y, each=length(x)), z1=as.vector(model_post1), z2=as.vector(model_post2))
ggplot(datPred, aes(x = LD1, y = LD2)) + geom_point(size=3, aes(pch=Class, col=Class)) + geom_contour(data=pr, aes(x=x, y=y, z=z1), breaks=c(0,.5)) + geom_contour(data=pr, aes(x=x, y=y, z=z2), breaks=c(0,.5))
first <- training[training$actual == 1,]
second <- training[training$actual == 2,]
third <- training[training$actual == 3,]
plot(first$x, first$y, col = "red", xlim = c(-5, 15), ylim = c(-15, 15), xlab = "x values", ylab = "y values")
points(second$x, second$y, col = "blue")
points(third$x, third$y, col = "green")
library(klaR)
partimat(all_vals ~ x + y, data = df_7a, method ="lda")
mdl <- qda(training$actual ~ x + y, data = testing)
pred_mdl <- predict(mdl, testing[,-3])
cm = as.matrix(table(Actual = training$actual, Predicted = pred_mdl$class))
cm
boundary <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
# if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
#
# z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
# contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
#         lwd = 2, levels = (1:(k-1))+.5)
#
# invisible(z)
}
model_7a <- qda(actual ~ x + y, data=training)
boundary(model_7a, training, class="actual")
boundary <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
#
# z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
# contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
#         lwd = 2, levels = (1:(k-1))+.5)
#
# invisible(z)
}
model_7a <- qda(actual ~ x + y, data=training)
boundary(model_7a, training, class="actual")
decisionplot <- function(model, data, class = NULL, showgrid = TRUE) {
cl <- data[,class]
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = 100)
ys <- seq(r[1,2], r[2,2], length.out = 100)
g <- cbind(rep(xs, each=100), rep(ys, time = 100))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
p <- predict(model, g, type = "class")
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = 100, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
model_7a <- qda(actual ~ x + y, data=training)
decisionplot(model_7a, training, class="actual")
decisionplot <- function(model, data, class = NULL) {
cl <- data[,class]
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = 100)
ys <- seq(r[1,2], r[2,2], length.out = 100)
g <- cbind(rep(xs, each=100), rep(ys, time = 100))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
p <- predict(model, g, type = "class")
if(is.list(p)) {
p <- p$class
p <- as.factor(p)
}
points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = 100, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
# invisible(z)
}
model_7a <- qda(actual ~ x + y, data=training)
decisionplot(model_7a, training, class="actual")
decisionplot <- function(model, data, class = NULL) {
cl <- data[,class]
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = 100)
ys <- seq(r[1,2], r[2,2], length.out = 100)
g <- cbind(rep(xs, each=100), rep(ys, time = 100))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
p <- predict(model, g, type = "class")
if(is.list(p)) {
p <- p$class
p <- as.factor(p)
}
points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = 100, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
# invisible(z)
}
model_7a <- qda(actual ~ x + y, data=training)
decisionplot(model_7a, training, class="actual")
decisionplot <- function(model, data, class = NULL, ...) {
cl <- data[,class]
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = 100)
ys <- seq(r[1,2], r[2,2], length.out = 100)
g <- cbind(rep(xs, each=100), rep(ys, time = 100))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
p <- predict(model, g, type = "class")
if(is.list(p)) {
p <- p$class
p <- as.factor(p)
}
points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = 100, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
# invisible(z)
}
model_7a <- qda(actual ~ x + y, data=training)
decisionplot(model_7a, training, class="actual")
decisionplot <- function(model, data, class = NULL, ...) {
cl <- data[,class]
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = 100)
ys <- seq(r[1,2], r[2,2], length.out = 100)
g <- cbind(rep(xs, each=100), rep(ys, time = 100))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
p <- predict(model, g, type = "class")
p <- p$class
p <- as.factor(p)
points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = 100, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE, lwd = 2, levels = (1:(k-1))+.5)
# invisible(z)
}
model_7a <- qda(actual ~ x + y, data=training)
decisionplot(model_7a, training, class="actual")
decisionplot <- function(model, data, class = NULL, ...) {
cl <- data[,class]
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = 100)
ys <- seq(r[1,2], r[2,2], length.out = 100)
g <- cbind(rep(xs, each=100), rep(ys, time = 100))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
p <- predict(model, g, type = "class")
p <- p$class
p <- as.factor(p)
points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = 100, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE, lwd = 2, levels = (1:(k-1))+.5)
}
ldamodel <- lda(actual ~ x + y, data=training)
decisionplot(ldamodel, training, class="actual")
qdamodel <- qda(actual ~ x + y, data = training)
decisionplot(qdamodel, training, class="actual")
